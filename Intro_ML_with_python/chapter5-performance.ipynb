{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mglearn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이진분류를 예로 들 때, accuracy, precision, recall, fallout, f-score등 다양한 방법들이 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불균형 데이터셋에서와 같이 accuracy만으론 평가가 힘들 경우,\n",
    "\n",
    "분류 임계값을 바꿔 불균형 데이터셋에 대해 예측을 fair하게 만드는 조치를 취하기도 한다\n",
    "\n",
    "(decision function, predict proba의 임계값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lower_threshold = svc.decision_function(X_test) > -.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그럼 분류 임계값이 달라짐에 따라 성능의 변화를 어떻게 확인할까\n",
    "> 1. Precision - Recall curve\n",
    "\n",
    "> 2. ROC curve (Recall - Fallout), AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그 전에, 다양한 성능지표들의 목표값은 어떻게 결정해야 할까?\n",
    "> 애플리케이션, 비즈니스 목표에 따라 다르다\n",
    "\n",
    "> 핵심은 사전에 정한 목표치를 유지하면서 성능을 올리는 것\n",
    "\n",
    "> 그 목표치를 __운영 포인트(operating point)__라고 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision - Recall Curve (354p)\n",
    "> 우측 상단으로 가면 좋다\n",
    "\n",
    "> Precision, Recall은 둘 다 높으면 좋음\n",
    "\n",
    "> 정량적으로 나타내는 방법 = average precision\n",
    "\n",
    "# ROC curve (357p)\n",
    "> 좌측 상단으로 가면 좋다\n",
    "\n",
    "> Recall은 높아야, Fallout은 낮아야 좋음\n",
    "\n",
    "> 정량적으로 나타내는 방법 = AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 만약 다중 분류를 평가하려면?\n",
    "> 앞서 살펴본 지표들 (OvR버전)\n",
    "\n",
    "> 만약 불균형 데이터셋이라면, f1-점수의 다중분류 버전을 수행\n",
    "1. macro\n",
    "2. weighted\n",
    "3. micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, pred, average = ABOVE_OPTPIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결론\n",
    "1. accuracy, precision, recall, fallout, f1-score, roc_auc, r2 등 다양한 평가지표를 활용하여 cross validaiton, parameter optimization을 수행할 수 있다\n",
    "2. 그리고 일단 모델링을 했다면 교차검증을 반드시 해야 한다\n",
    "3. 머신러닝 모델 선택과 이에 대해 어느 평가지표를 사용할 지도 중요하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용할 수 있는 평가지표는 어디서 확인하나요?\n",
    "> 366 p 참고!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
